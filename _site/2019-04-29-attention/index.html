<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>Attentionチュートリアル</title>

  
  <meta name="author" content="deeplab">
  

  <meta name="description" content="目的

  様々な場面で登場するようになったAttention機構を実装ベースで理解する．
  実際に機械翻訳のタスクを実行し，従来のSeq2Seqモデルと比較してTransformerの精度の高さを確認する


概要

2017年 “Attention Is All You Need” という論文が Googleから発表され，機械翻訳の分野で既存のRNN，LSTM，GRU等のモデルを大きく上回る結果となりました．提案されたTransformerモデルは，従来のRNNやCNNを使用せず，Attention機構のみを用いるSeq2Seqモデルです．今回は，このAttention機構を実装ベースで理解します．RNNと異なり並列計算が可能で計算が高速な上，Self-Attentionと呼ばれる機構を用いることにより，局所的な情報しか参照できないCNNと異なり，系列内の任意の位置情報を参照することができます．現在，自然言語処理のデファクトスタンダードとなっているBERTはこのTransformerに端を発しています．さらに，類似手法が，画像認識，生成モデル，音声認識などの分野で幅広く利用されています．このようにAttentionの動作原理を理解することは，深層学習分野において極めて重要です．

実施期間・日時

場所: 友人宅 
日時: 2019年4月29日 10時 - 12時

参考資料

Attention is All You Need [A. Vaswani+, NeurIPS’17] 
https://arxiv.org/abs/1706.03762">

  

  

  <link rel="alternate" type="application/rss+xml" title="deeplab" href="http://localhost:4000/feed.xml">

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="deeplab">
  <meta property="og:title" content="Attentionチュートリアル">
  <meta property="og:description" content="目的

  様々な場面で登場するようになったAttention機構を実装ベースで理解する．
  実際に機械翻訳のタスクを実行し，従来のSeq2Seqモデルと比較してTransformerの精度の高さを確認する


概要

2017年 “Attention Is All You Need” という論文が Googleから発表され，機械翻訳の分野で既存のRNN，LSTM，GRU等のモデルを大きく上回る結果となりました．提案されたTransformerモデルは，従来のRNNやCNNを使用せず，Attention機構のみを用いるSeq2Seqモデルです．今回は，このAttention機構を実装ベースで理解します．RNNと異なり並列計算が可能で計算が高速な上，Self-Attentionと呼ばれる機構を用いることにより，局所的な情報しか参照できないCNNと異なり，系列内の任意の位置情報を参照することができます．現在，自然言語処理のデファクトスタンダードとなっているBERTはこのTransformerに端を発しています．さらに，類似手法が，画像認識，生成モデル，音声認識などの分野で幅広く利用されています．このようにAttentionの動作原理を理解することは，深層学習分野において極めて重要です．

実施期間・日時

場所: 友人宅 
日時: 2019年4月29日 10時 - 12時

参考資料

Attention is All You Need [A. Vaswani+, NeurIPS’17] 
https://arxiv.org/abs/1706.03762">

  
  <meta property="og:image" content="http://localhost:4000/assets/img/logo.jpg">
  

  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="deeplab">
  <meta property="og:article:published_time" content="2019-04-29T00:00:00+09:00">
  <meta property="og:url" content="http://localhost:4000/2019-04-29-attention/">
  <link rel="canonical" href="http://localhost:4000/2019-04-29-attention/">
  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="http://localhost:4000/">deeplab</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/members">Members</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/_posts/projects">Projects</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/_posts/seminars">Seminars</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/_posts/implementation">Implementation</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/contact">Contact</a>
          </li></ul>
  </div>

  

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="http://localhost:4000/">
          <img alt="Navigation bar avatar" class="avatar-img" src="/assets/img/logo.jpg" />
        </a>
      </div>
    </div>
  

</nav>


  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Attentionチュートリアル</h1>
          

          
            <span class="post-meta">Posted on April 29, 2019</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      

      

      <article role="main" class="blog-post">
        <h2 id="目的">目的</h2>
<ul>
  <li>様々な場面で登場するようになったAttention機構を実装ベースで理解する．</li>
  <li>実際に機械翻訳のタスクを実行し，従来のSeq2Seqモデルと比較してTransformerの精度の高さを確認する</li>
</ul>

<h2 id="概要">概要</h2>

<p>2017年 “Attention Is All You Need” という論文が Googleから発表され，機械翻訳の分野で既存のRNN，LSTM，GRU等のモデルを大きく上回る結果となりました．提案されたTransformerモデルは，従来のRNNやCNNを使用せず，Attention機構のみを用いるSeq2Seqモデルです．今回は，このAttention機構を実装ベースで理解します．RNNと異なり並列計算が可能で計算が高速な上，Self-Attentionと呼ばれる機構を用いることにより，局所的な情報しか参照できないCNNと異なり，系列内の任意の位置情報を参照することができます．現在，自然言語処理のデファクトスタンダードとなっているBERTはこのTransformerに端を発しています．さらに，類似手法が，画像認識，生成モデル，音声認識などの分野で幅広く利用されています．このようにAttentionの動作原理を理解することは，深層学習分野において極めて重要です．</p>

<h2 id="実施期間日時">実施期間・日時</h2>

<p>場所: 友人宅 <br />
日時: 2019年4月29日 10時 - 12時</p>

<h2 id="参考資料">参考資料</h2>

<p>Attention is All You Need [A. Vaswani+, NeurIPS’17] <br />
https://arxiv.org/abs/1706.03762</p>

      </article>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/tags#Implementation">Implementation</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        
        <li class="page-item next">
          <a class="page-link" href="/2020-04-05-image-recog/" data-toggle="tooltip" data-placement="top" title="画像認識勉強会">Next Post &rarr;</a>
        </li>
        
      </ul>
      
  
  
  

  




    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"></ul>

      
      <p class="copyright text-muted">
      
        deeplab
        &nbsp;&bull;&nbsp;
      
      2020

      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
