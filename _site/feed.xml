<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    
    <title>deeplab</title>
    
    
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
    
      <item>
        <title>GANとStyleGAN(v2)のサーベイ</title>
        <description>
          
          目的 GANの基本構造の理解する． StyleGANおよびStyleGANv2の構造を理解する． 現実世界の画像を潜在空間に落とし込む手法を理解する． 概要 敵対的生成ネットワーク(GAN)は深層生成モデルの一つであり，適切に学習すれば，学習訓練データ集合に無く，かつ訓練データと見分けがつかないよいうなデータを生成することができます．本セミナーでは，近年世界に衝撃を与えたGANであるStyleGANを中心にして，GANの構造を説明します．さらに発展的な話題として，現実画像を潜在変数空間に落とし込む手法も説明します． 発表日時 場所:  オンライン (Zoom) 日時: 2020年5月10日10時 - 12時 参考資料 Generative Adversarial Networks [I. Goodfellow+, NeurIPS’17] https://arxiv.org/abs/1406.2661 Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks [A. Radford+, ICLR’16] https://arxiv.org/abs/1511.06434 Wasserstein GAN [M. Arjovsky+, ICML’17] https://arxiv.org/abs/1701.07875 A Style-Based Generator Architecture for Generative Adversarial Networks [T....
        </description>
        <pubDate>Sun, 10 May 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-05-10-gan/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-05-10-gan/</guid>
      </item>
    
      <item>
        <title>画像認識勉強会</title>
        <description>
          
          目的

  画像処理全般の体系的な知識を身につける．


概要

画像認識とは，画像に写っているものを計算機が識別する技術のことです．画像認識は，パターン認識や機械学習，応用先の経験的な知識など，扱う内容が多く，また，人工知能ブームに伴い，広大かつ膨大な画像認識に関する手法が提案されてきました．本勉強会では画像認識における知識を体系的に身につけるべく，古典的手法，それらをまとめてモデル化するニューラルネットワーク，物体検出，セマンティックセグメンテーション，画像生成などを扱います．

第1章:画像認識の概要 
第2章:局所特徴 
第3章:統計的特徴抽出 
第4章:コーディングとプーリング 
第5章:分類 
第6章:畳み込みニューラルネットワーク 
第7章:物体検出 
第8章:インスタンス認識と検索 
第9章:さらなる話題

実施期間・日時

場所: 東工大 
日時: 2020年3月 - 4月, 毎週日曜日13時 - 15時

参考資料

原田達也．『画像認識』, 講談社．

        </description>
        <pubDate>Sun, 05 Apr 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-04-05-image-recog/</link>
        <guid isPermaLink="true">http://localhost:4000/2020-04-05-image-recog/</guid>
      </item>
    
      <item>
        <title>Attentionチュートリアル</title>
        <description>
          
          目的

  様々な場面で登場するようになったAttention機構を実装ベースで理解する．
  実際に機械翻訳のタスクを実行し，従来のSeq2Seqモデルと比較してTransformerの精度の高さを確認する


概要

2017年 “Attention Is All You Need” という論文が Googleから発表され，機械翻訳の分野で既存のRNN，LSTM，GRU等のモデルを大きく上回る結果となりました．提案されたTransformerモデルは，従来のRNNやCNNを使用せず，Attention機構のみを用いるSeq2Seqモデルです．今回は，このAttention機構を実装ベースで理解します．RNNと異なり並列計算が可能で計算が高速な上，Self-Attentionと呼ばれる機構を用いることにより，局所的な情報しか参照できないCNNと異なり，系列内の任意の位置情報を参照することができます．現在，自然言語処理のデファクトスタンダードとなっているBERTはこのTransformerに端を発しています．さらに，類似手法が，画像認識，生成モデル，音声認識などの分野で幅広く利用されています．このようにAttentionの動作原理を理解することは，深層学習分野において極めて重要です．

実施期間・日時

場所: 友人宅 
日時: 2019年4月29日 10時 - 12時

参考資料

Attention is All You Need [A. Vaswani+, NeurIPS’17] 
https://arxiv.org/abs/1706.03762

        </description>
        <pubDate>Mon, 29 Apr 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-04-29-attention/</link>
        <guid isPermaLink="true">http://localhost:4000/2019-04-29-attention/</guid>
      </item>
    
  </channel>
</rss>
